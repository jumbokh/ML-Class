{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Ch12HW 整合預測器ensemble.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/ML-Class/blob/main/notebooks/Ch12HW_%E6%95%B4%E5%90%88%E9%A0%90%E6%B8%AC%E5%99%A8ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9UPFoGbtpLH"
      },
      "source": [
        "## 作業\n",
        "1. 請用鳶尾花全部資料來做分析，並用以下參數進行網格搜尋。\n",
        "\n",
        "param_grid = [  \n",
        "    {'model':[RandomForestClassifier()], 'model\\_\\_n\\_estimators': [100, 500]},   \n",
        "    {'model':[AdaBoostClassifier()], 'model\\_\\_n\\_estimators': [100, 500],  \n",
        "     'model\\_\\_base\\_estimator':[None, RandomForestClassifier(max_depth=1)]},  \n",
        "    {'model':[XGBClassifier()], 'model\\_\\_n\\_estimators': [100, 500]},    \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz48hRy4tpLK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "# plt.rcParams['font.sans-serif'] = ['DFKai-sb'] \n",
        "# plt.rcParams['axes.unicode_minus'] = False\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy52St2ftpLL",
        "outputId": "9775414b-0762-4020-d069-12c93397e3db"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
        "df['target'] = iris['target']\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                   test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model_pl = Pipeline([\n",
        "    ('preprocess', StandardScaler()),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {'model':[RandomForestClassifier()], \n",
        "     'model__n_estimators': [100, 500]},\n",
        "    {'model':[AdaBoostClassifier()], \n",
        "     'model__n_estimators': [100, 500],\n",
        "     'model__base_estimator':[None, RandomForestClassifier(max_depth=1, random_state=42)]},\n",
        "    {'model':[XGBClassifier()], \n",
        "     'model__n_estimators': [100, 500]},\n",
        "]\n",
        "gs = GridSearchCV(model_pl, param_grid=param_grid,\n",
        "                  cv=5, return_train_score=True)\n",
        "gs.fit(X_train, y_train)\n",
        "y_pred = gs.best_estimator_.predict(X_test)\n",
        "score = gs.best_estimator_.score(X_test, y_test)\n",
        "print('最佳模型', gs.best_estimator_)\n",
        "print('訓練集交叉驗證的最佳結果', gs.best_score_)\n",
        "print('測試集的結果', score)\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:37:55] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:55] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:56] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:56] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:56] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:56] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:56] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:37:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "最佳模型 Pipeline(steps=[('preprocess', StandardScaler()),\n",
            "                ('model', AdaBoostClassifier(n_estimators=100))])\n",
            "訓練集交叉驗證的最佳結果 0.93\n",
            "測試集的結果 0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19,  0,  0],\n",
              "       [ 0, 15,  0],\n",
              "       [ 0,  5, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biRyZGkOtpLN"
      },
      "source": [
        "2. 請用鐵達尼號資料來做分析，並用以下參數進行網格搜尋。\n",
        "\n",
        "param_grid = [  \n",
        "    {'model':[RandomForestClassifier()], 'model\\_\\_n\\_estimators': [100, 500]},   \n",
        "    {'model':[AdaBoostClassifier()], 'model\\_\\_n\\_estimators': [100, 500],  \n",
        "     'model\\_\\_base\\_estimator':[None, RandomForestClassifier(max_depth=1)]},  \n",
        "    {'model':[XGBClassifier()], 'model\\_\\_n\\_estimators': [100, 500]},    \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUqfvt3KtpLN",
        "outputId": "db1c8435-9873-4ef6-ea59-6ce888bc4dad"
      },
      "source": [
        "df = pd.read_csv('titanic_train.csv')\n",
        "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                   test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "num_pl = make_pipeline(\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    StandardScaler()\n",
        ")\n",
        "cat_pl = make_pipeline(\n",
        "    SimpleImputer(strategy='most_frequent'),\n",
        "    OneHotEncoder()\n",
        ")\n",
        "\n",
        "data_pl = ColumnTransformer([\n",
        "    ('num_pl', num_pl, ['Age','Fare', 'SibSp','Parch','Pclass']),\n",
        "    ('cat_pl', cat_pl, ['Sex','Embarked'])\n",
        "])\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model_pl = Pipeline([\n",
        "    ('preprocess', data_pl),\n",
        "    ('model', LogisticRegression())\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {'model':[RandomForestClassifier()], \n",
        "     'model__n_estimators': [100, 500]},\n",
        "    {'model':[AdaBoostClassifier()], \n",
        "     'model__n_estimators': [100, 500],\n",
        "     'model__base_estimator':[None, RandomForestClassifier(max_depth=1, random_state=42)]},\n",
        "    {'model':[XGBClassifier()], \n",
        "     'model__n_estimators': [100, 500]},\n",
        "]\n",
        "\n",
        "gs = GridSearchCV(model_pl, param_grid=param_grid,\n",
        "                  cv=7, return_train_score=True)\n",
        "gs.fit(X_train, y_train)\n",
        "y_pred = gs.best_estimator_.predict(X_test)\n",
        "score = gs.best_estimator_.score(X_test, y_test)\n",
        "print('最佳模型', gs.best_params_['model'])\n",
        "print('訓練集交叉驗證的最佳結果', gs.best_score_)\n",
        "print('測試集的結果', score)\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19:50:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[19:50:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "最佳模型 AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=1,\n",
            "                                                         random_state=42),\n",
            "                   n_estimators=500)\n",
            "訓練集交叉驗證的最佳結果 0.8072112565956616\n",
            "測試集的結果 0.7898305084745763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[144,  31],\n",
              "       [ 31,  89]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXY69P5stpLO"
      },
      "source": [
        "3. 請用軟投票組合器來進行鐵達尼號存活預測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAZcfOvXtpLP",
        "outputId": "16179384-1a52-4e43-d94b-89787d7584cb"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_pl_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "model_pl_svc = make_pipeline(StandardScaler(), SVC(probability=True))\n",
        "model_pl_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
        "model_pl_tree = make_pipeline(DecisionTreeClassifier(max_depth=10))\n",
        "\n",
        "vc = VotingClassifier([\n",
        "    ('lr', model_pl_lr),    \n",
        "    ('svc', model_pl_svc), \n",
        "    ('tree', model_pl_tree), \n",
        "    ('knn', model_pl_knn)], \n",
        "    voting='soft', weights=[2, 3, 1, 1])\n",
        "\n",
        "model_pl = make_pipeline(data_pl, vc)\n",
        "model_pl.fit(X_train, y_train)\n",
        "y_pred = model_pl.predict(X_test)\n",
        "\n",
        "train_score = model_pl.score(X_train, y_train)\n",
        "test_score = model_pl.score(X_test, y_test)\n",
        "print('訓練集的預測結果', train_score)\n",
        "print('測試集的預測結果', test_score)\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "訓練集的預測結果 0.8557046979865772\n",
            "測試集的預測結果 0.8271186440677966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[161,  14],\n",
              "       [ 37,  83]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}